{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 一人分の同時分布\n",
    "\n",
    "確率変数の定義域を以下のように定める\n",
    "$\n",
    "h \\in {R_A, R_B,R_{AB}, R_O}\n",
    "\\ $, $\n",
    "g \\in {aa, ao, bb, bo, ab, oo}\n",
    "$,\n",
    "\n",
    "このとき、一人分の同時分布は\n",
    "\n",
    "$$\n",
    "p(h,g;\\boldsymbol\\theta) = \\begin{cases}\n",
    "    \\theta_A^2 & (h=R_A, g=aa) \\\\\n",
    "    2\\theta_A\\theta_O & (h=R_A, g=ao) \\\\\n",
    "    \\theta_B^2 & (h=R_B, g=bb) \\\\\n",
    "    2\\theta_B\\theta_O & (h=R_B, g=bo) \\\\\n",
    "    2\\theta_A\\theta_B & (h=R_{AB}, g=ab) \\\\\n",
    "    \\theta_O^2 & (h=R_O, g=oo) \\\\\n",
    "    0 & (\\text{otherwise})\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "図で示すとわかりやすく、その場合は以下のようになる。\n",
    "\n",
    "| h\\g | aa | ao | bb | bo | ab | oo |\n",
    "|:-----------|:------------:|:------------:|:-----------:|:------------:|:------------:|:------------:|\n",
    "| $R_A$ | $\\theta_A^2$ | $2\\theta_A\\theta_O$ | 0 | 0 | 0 | 0 |\n",
    "| $R_B$  | 0 | 0 | $\\theta_B^2$ | $2\\theta_B\\theta_O$ | 0 | 0 |\n",
    "| $R_{AB}$ | 0 | 0 | 0 | 0 | $2\\theta_A\\theta_B$ | 0 |\n",
    "| $R_O$ | 0 | 0 | 0 | 0 | 0 |$\\theta_O^2$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. i番目の人について、$h_i, g_i$としたときに、すべての人に対して、全確率変数の同時分布を求める\n",
    "\n",
    "尤度を求めるようなもの。ここで、gの取りうるすべてに対して、その人数を$\\{N_{aa},N_{ao},N_{bb},N_{bo},N_{ab},N_{oo}\\}$とする。また、全体でN人いるとおく。これを用いると、回答は以下のようになる。\n",
    "\n",
    "$$\n",
    "p({\\bf h},{\\bf g} ; \\boldsymbol{\\theta}) = \\frac{N!}{N_{aa}！N_{ao}！N_{bb}！N_{bo}！N_{ab}！N_{oo}！} (\\theta_A^2)^{N_{aa}}(2\\theta_A\\theta_O)^{N_{ao}}(\\theta_B^2)^{N_{bb}}(2\\theta_B\\theta_O)^{N_{bo}}(2\\theta_A\\theta_B)^{N_{ab}}(\\theta_O^2)^{N_{oo}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. EMアルゴリズムの更新式\n",
    "\n",
    "2の式に対して負の対数を取ると\n",
    "\n",
    "$$\n",
    "-\\log(p({\\bf h},{\\bf g} ; \\boldsymbol{\\theta})) = -\\left\\{\\log(\\frac{N!}{N_{aa}！N_{ao}！N_{bb}！N_{bo}！N_{ab}！N_{oo}！}) + N_{aa}\\log(\\theta_A^2) + N_{ao}\\log(2\\theta_A\\theta_O) + N_{bb} \\log(\\theta_B^2) + N_{bo}\\log(2\\theta_B\\theta_O) + N_{ab}\\log(2\\theta_A\\theta_B) + N_{oo}\\log(\\theta_O^2)\\right\\}\n",
    "$$\n",
    "\n",
    "となる。これについて、$p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)$で平均を取りたいが、この分布を陽に計算することは困難である。そこでもう少し式変形をしてみる。\n",
    "\n",
    "$$\n",
    "\\left< -\\log(p({\\bf h},{\\bf g} ; \\boldsymbol{\\theta}))\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}= \n",
    "-\\left\\{\n",
    "\\left< \\log(\\frac{N!}{N_{aa}！N_{ao}！N_{bb}！N_{bo}！N_{ab}！N_{oo}！}) \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\\\\\n",
    "+\\left< N_{aa}\\log(\\theta_A^2) \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\left<  N_{ao}\\log(2\\theta_A\\theta_O) \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)} \n",
    "+\\left< N_{bb} \\log(\\theta_B^2)\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\left< N_{bo}\\log(2\\theta_B\\theta_O)\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\left< N_{ab}\\log(2\\theta_A\\theta_B)\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\left< N_{oo}\\log(\\theta_O^2)\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left< -\\log(p({\\bf h},{\\bf g} ; \\boldsymbol{\\theta}))\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}= \n",
    "-\\left\\{\n",
    "\\left< \\log(\\frac{N!}{N_{aa}！N_{ao}！N_{bb}！N_{bo}！N_{ab}！N_{oo}！}) \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\\\\\n",
    "+\\log(\\theta_A^2)\\left< N_{aa} \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\log(2\\theta_A\\theta_O) \\left< N_{ao}\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)} \n",
    "+\\log(\\theta_B^2)\\left< N_{bb}\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\log(2\\theta_B\\theta_O)\\left< N_{bo}\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\log(2\\theta_A\\theta_B)\\left< N_{ab}\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\n",
    "+\\log(\\theta_O^2)\\left< N_{oo}\\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}\\right\\}\n",
    "$$\n",
    "\n",
    "ここで、$\\left< N_{aa} \\right>_{p({\\bf g} | {\\bf h} ; \\boldsymbol \\theta)}$ 等は観測データと現在のパラメーターより、以下のようにわかる。\n",
    "EMalgorithmで扱うiterationの添字を右型にカッコつきで書く。 k th iterationではその時のパラメーターを用いて以下のように、書くことができる。\n",
    "\n",
    "$$\n",
    "N_{aa}^{(k)} = NR_A \\frac{\\theta_A^{(k)^2}}{\\theta_A^{(k)^2} + 2\\theta_A^{(k)^2}\\theta_O^{(k)^2}} \\\\\n",
    "N_{ao}^{(k)} = NR_A \\frac{2\\theta_A^{(k)^2}\\theta_O^{(k)^2}}{\\theta_A^{(k)^2} + 2\\theta_A^{(k)^2}\\theta_O^{(k)^2}} \\\\\n",
    "N_{bb}^{(k)} = NR_B \\frac{\\theta_B^{(k)^2}}{\\theta_B^{(k)^2} + 2\\theta_B^{(k)^2}\\theta_O^{(k)^2}} \\\\\n",
    "N_{bo}^{(k)} = NR_B \\frac{2\\theta_B^{(k)^2}\\theta_O^{(k)^2}}{\\theta_B^{(k)^2} + 2\\theta_B^{(k)^2}\\theta_O^{(k)^2}} \\\\\n",
    "N_{ab} = NR_{AB} \\\\\n",
    "N_{oo} = NR_O\n",
    "$$\n",
    "\n",
    "これがいわゆるE-stepである。\n",
    "\n",
    "M-stepの更新は$N_{aa}^{(k)}$等を固定して、以下の損失関数を最小化する$\\theta_A, \\theta_B, \\theta_O$を$\\theta_A^{(k+1)}, \\theta_B^{(k+1)}, \\theta_O^{(k+1)}$とする。\n",
    "\n",
    "$$\n",
    "loss(\\theta_A, \\theta_B, \\theta_O)= \n",
    "-\\left\\{\n",
    "\\log(\\theta_A^2)N_{aa}^{(k)}\n",
    "+\\log(2\\theta_A\\theta_O) N_{ao}^{(k)}\n",
    "+\\log(\\theta_B^2) N_{bb}^{(k)}\n",
    "+\\log(2\\theta_B\\theta_O) N_{bo}^{(k)}\n",
    "+\\log(2\\theta_A\\theta_B)N_{ab}\n",
    "+\\log(\\theta_O^2)N_{oo}\\right\\}\n",
    "$$\n",
    "\n",
    "(最小化の上ではNは必要ないのでNが与えられなくてもthetaを計算できる)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. $\\theta$を最尤解を求める\n",
    "M-stepに関しては勾配法で損失関数を最小化する$\\theta$を見つける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(2019)\n",
    "theta=torch.rand(2, requires_grad=True)\n",
    "Ra,Rb,Rab,Ro=0.4,0.2,0.1,0.3\n",
    "\n",
    "def expectation(t_a,t_b,t_o):\n",
    "    Naa=Ra*((t_a**2)/(2*t_a*t_o + t_a**2))\n",
    "    Nao=Ra*((2*t_a*t_o)/(2*t_a*t_o + t_a**2))\n",
    "    Nbb=Rb*((t_b**2)/(2*t_b*t_o + t_b**2))\n",
    "    Nbo=Rb*((2*t_b*t_o)/(2*t_b*t_o + t_b**2))\n",
    "    Nab=Rab;Noo=Ro\n",
    "    return [Naa,Nao,Nbb,Nbo,Nab,Noo]\n",
    "\n",
    "def loss_fn(t_a,t_b,t_o, Ns):\n",
    "    Naa,Nao,Nbb,Nbo,Nab,Noo =Ns\n",
    "    return -(Naa*torch.log(t_a**2)+Nao*torch.log(2*t_a*t_o)\n",
    "            +Nbb*torch.log(t_b**2)+Nbo*torch.log(2*t_b*t_o)\n",
    "            +Nab*torch.log(2*t_a*t_b)+Noo*torch.log(t_o**2))\n",
    "\n",
    "def argmin(theta,Ns,lr=0.01,n_epochs=1000):\n",
    "    for e in range(n_epochs):\n",
    "        #損失関数の定義\n",
    "        loss=loss_fn(theta[0],theta[1],(1-theta[0]-theta[1]), Ns)\n",
    "        # SGDで最適化\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            theta=theta - lr*theta.grad\n",
    "        theta.requires_grad_()\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2519, 0.5308], requires_grad=True)\n",
      "tensor([0.1877, 0.3039], requires_grad=True)\n",
      "tensor([0.2060, 0.1990], requires_grad=True)\n",
      "tensor([0.2222, 0.1563], requires_grad=True)\n",
      "tensor([0.2310, 0.1443], requires_grad=True)\n",
      "tensor([0.2349, 0.1414], requires_grad=True)\n",
      "tensor([0.2364, 0.1407], requires_grad=True)\n",
      "tensor([0.2371, 0.1404], requires_grad=True)\n",
      "tensor([0.2373, 0.1403], requires_grad=True)\n",
      "tensor([0.2374, 0.1403], requires_grad=True)\n",
      "\n",
      "theta_a:0.2374424934387207\n",
      "theta_b:0.14026571810245514\n",
      "theta_o:0.6222918033599854\n"
     ]
    }
   ],
   "source": [
    "for iloop in range(100):\n",
    "    #E-step\n",
    "    Ns=expectation(theta[0],theta[1],(1-theta[0]-theta[1]))\n",
    "    \n",
    "    #M-step\n",
    "    theta=argmin(theta,Ns)\n",
    "    \n",
    "    if iloop % 10 ==0:\n",
    "        print(theta)\n",
    "\n",
    "print('\\ntheta_a:{}\\ntheta_b:{}\\ntheta_o:{}'.format(theta[0],theta[1],(1-theta[0]-theta[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2247, 0.4733], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "loss(\\theta_A, \\theta_B, \\theta_O) = -\\log(\\Gamma(N_{aa}+1))-\\log(\\Gamma(N_{ao}+1))-\\log(\\Gamma(N_{bb}+1))-\\log(\\Gamma(N_{bo}+1))-\\log(\\Gamma(N_{ab}+1))-\\log(\\Gamma(N_{oo}+1)) \\\\ + N_{aa}\\log(\\theta_A^2) + N_{ao}\\log(2\\theta_A\\theta_O) + N_{bb} \\log(\\theta_B^2) + N_{bo}\\log(2\\theta_B\\theta_O) + N_{ab}\\log(2\\theta_A\\theta_B) + N_{oo}\\log(\\theta_O^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別解、$N_aa$等を$theta$に関する関数だと解釈して、勾配法で2の負の対数尤度を最小化する。\n",
    "ただしNについて答えが変化するので、Nを日本の人口である($10^8$で実験を行った。)\n",
    "\n",
    "損失関数\n",
    "$$\n",
    "loss(\\theta_A, \\theta_B, \\theta_O) = -\\log(\\Gamma(N_{aa}+1))-\\log(\\Gamma(N_{ao}+1))-\\log(\\Gamma(N_{bb}+1))-\\log(\\Gamma(N_{bo}+1))-\\log(\\Gamma(N_{ab}+1))-\\log(\\Gamma(N_{oo}+1)) \\\\ - N_{aa}\\log(\\theta_A^2) - N_{ao}\\log(2\\theta_A\\theta_O) - N_{bb} \\log(\\theta_B^2) - N_{bo}\\log(2\\theta_B\\theta_O) - N_{ab}\\log(2\\theta_A\\theta_B) - N_{oo}\\log(\\theta_O^2)\n",
    "$$\n",
    "\n",
    "実装上においてはガンマ関数をスターリングの近似を用いて以下のように定義した。\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別解、$N_aa$等を$theta$に関する関数だと解釈して、勾配法で2の負の対数尤度を最小化する。\n",
    "ただしNについて答えが変化するので、Nを日本の人口である($10^8$で実験を行った。)\n",
    "\n",
    "損失関数\n",
    "$$\n",
    "loss(\\theta_A, \\theta_B, \\theta_O) = -\\log(\\Gamma(N_{aa}+1))-\\log(\\Gamma(N_{ao}+1))-\\log(\\Gamma(N_{bb}+1))-\\log(\\Gamma(N_{bo}+1))-\\log(\\Gamma(N_{ab}+1))-\\log(\\Gamma(N_{oo}+1)) \\\\ + N_{aa}\\log(\\theta_A^2) + N_{ao}\\log(2\\theta_A\\theta_O) + N_{bb} \\log(\\theta_B^2) + N_{bo}\\log(2\\theta_B\\theta_O) + N_{ab}\\log(2\\theta_A\\theta_B) + N_{oo}\\log(\\theta_O^2)\n",
    "$$\n",
    "\n",
    "実装上においてはガンマ関数をスターリングの近似を用いて以下のように定義した。\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_a$,\n",
    "$\\theta_b$,\n",
    "をthetaとおく。ただし$\\theta_o$=(1-$\\theta_a$-$\\theta_b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9203, 3.6476], grad_fn=<LgammaBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lgamma(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(2019)\n",
    "torch.manual_seed(2000)\n",
    "theta=torch.rand(2, requires_grad=True)\n",
    "Ra,Rb,Rab,Ro=0.4,0.2,0.1,0.3\n",
    "\n",
    "def log_likelyhood(t_a,t_b,t_o):\n",
    "    Naa=Ra*((t_a**2)/(2*t_a*t_o + t_a**2))\n",
    "    Nao=Ra*((2*t_a*t_o)/(2*t_a*t_o + t_a**2))\n",
    "    Nbb=Rb*((t_b**2)/(2*t_b*t_o + t_b**2))\n",
    "    Nbo=Rb*((2*t_b*t_o)/(2*t_b*t_o + t_b**2))\n",
    "    Nab=Rab;Noo=Ro\n",
    "    torch.ga\n",
    "    return -(Naa*torch.log(t_a**2)+Nao*torch.log(2*t_a*t_o)\n",
    "            +Nbb*torch.log(t_b**2)+Nbo*torch.log(2*t_b*t_o)\n",
    "            +Nab*torch.log(2*t_a*t_b)+Noo*torch.log(t_o**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0678, 0.0457], requires_grad=True)\n",
      "tensor([0.2021, 0.1365], requires_grad=True)\n",
      "tensor([0.2255, 0.1420], requires_grad=True)\n",
      "tensor([0.2329, 0.1414], requires_grad=True)\n",
      "tensor([0.2357, 0.1408], requires_grad=True)\n",
      "tensor([0.2368, 0.1405], requires_grad=True)\n",
      "tensor([0.2372, 0.1404], requires_grad=True)\n",
      "tensor([0.2374, 0.1403], requires_grad=True)\n",
      "tensor([0.2374, 0.1403], requires_grad=True)\n",
      "tensor([0.2374, 0.1403], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "for epoch in range(1000):\n",
    "    #損失関数の定義\n",
    "    t_a,t_b=theta\n",
    "    loss=log_likelyhood(theta[0],theta[1],(1-theta[0]-theta[1]))    \n",
    "    \n",
    "    # SGDで最適化\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        theta=theta - lr*theta.grad\n",
    "\n",
    "    theta.requires_grad_()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_a:0.2374601662158966\n",
      "theta_b:0.14025960862636566\n",
      "theta_o:0.6222802400588989\n"
     ]
    }
   ],
   "source": [
    "print('theta_a:{}\\ntheta_b:{}\\ntheta_o:{}'.format(theta[0],theta[1],(1-theta[0]-theta[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
