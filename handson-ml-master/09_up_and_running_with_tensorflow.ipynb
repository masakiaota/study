{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 9 – Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denkenhii/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.variables.Variable'> <class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "print(type(x), type(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初期化して実行するちょっと楽な書き方\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# いちいち変数を初期化するのはめんどくさい\n",
    "# 更に楽な書き方\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# jupyterではInterctiveSessionの方が便利でwithブロックがいらない。\n",
    "# ただし、セッション終了時にきちんと閉じなければいけない。\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs\n",
    "### 9.3 グラフの管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()\n",
    "# 製作したノード(x1)は自動的にデフォルトグラフに追加される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph\n",
    "# 複数の独立したグラフを作りたい場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x2はgraphに追加されたのであってdefaultグラフには追加されていない\n",
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 ノードの値のライフサイクル\n",
    "tfは自動的にノードが依存しているノードを判断し、それらを評価する。\n",
    "ノードを再利用するときに、一度評価したノードも、計算をしなおすので書き方によっては計算量が多くなってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "# 悪い例、wとxはそれぞれ二回評価されている\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# こう書けば一回のグラフ実行でyもzも評価される\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.71450067\n",
      "Epoch 200 MSE = 0.5667049\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.53650916\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.34770885],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145283],\n",
       "       [-0.6375278 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the `gradients = ...` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21253741\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296286 ]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.3057109 ]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes\n",
    "### 9.7 訓練アルゴリズムへのデータの供給\n",
    "ミニバッチ勾配降下法を実装するには少しずつ学習データを与える必要がある。実装する前に**プレースホルダー**を使ってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\") # 列数のみ指定(説明変数＋ダミー変数)\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") # ターゲットベクターを格納するためのプレースホルダー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#グラフの定義\n",
    "# 係数ベクトル、最初は-1~1までの乱数を降っておく\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\") #np.dot(X,theta)に等しい\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\") # 与えたリストに入っている数値の平均値を求める関数\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# バッチサイズの定義\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    '''\n",
    "    全データからサンプリングしてランダムにミニバッチを取得するための関数。\n",
    "    '''\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # 0以上m未満の整数がbatch_size分のadarrayで返ってくる。\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model\n",
    "モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "# ノードの準備\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "# グラフの構築, 勾配降下法でthetaを求める。\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# ここでセーブノードを作ってる\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            # セーブ 合計で10回セーブしているが途中の結果もセーブされているのだろうか…？\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    #最後の結果もセーブ\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 復元\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)\n",
    "# 前に求めたベストなweightsは,restoreしたweightsと同じ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weightsという名前でthetaを保存することができる。\n",
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでノードの値を保存していたが、グラフの構造もリストアすることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "# ノードの構築\n",
    "# imort_meta_graphをつかってグラフ構造を読み込むことができる。\n",
    "saver = tf.train.import_meta_graph(\"./tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") # not shown in the book\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored) #allclose、近似的にndarrayが同じかどうか調べている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the graph within Jupyter, we will use a TensorBoard server available online at https://tensorboard.appspot.com/ (so this will not work if you do not have Internet access).  As far as I can tell, this code was originally written by Alex Mordvintsev in his [DeepDream tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb). Alternatively, you could use a tool like [tfgraphviz](https://github.com/akimach/tfgraphviz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_graph_in_jupyter import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 20640.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;sub&quot;\\n  input: &quot;gradients/Square_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/Square_grad/Mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Square_grad/Mul_1&quot;\\n  input: &quot;^gradients/sub_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Square_grad/Mul_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Square_grad/Mul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") # 現在の時刻を特定のフォーマットに\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "# 今まで通りのグラフの定義\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mseを評価してsummaryとする\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "# ログティレクトリーサマリーを書き込むためのfilewriterを作っている。\n",
    "# logdirは相対パスで指定されるの（基本的にカレントディレクトリの配下）\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0: # 計10回書き込んでいる。\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\n\\n\\x03MSE\\x15&w\\x1e?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_str\n",
    "#こんなものがfile_writerによっってサマリーのファイルに書き込まれている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes\n",
    "NNだとノード数が多すぎてわけがわからなくなるので名前スコープというのを使うと便利"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# 保存ディレクトリといままでのグラフ\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#errorと二乗誤差の計算をlossという名前でまとめた\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.0703337 ]\n",
      " [ 0.8637145 ]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.8030471 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.1784179106002547&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 9\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;loss/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/Square&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/mse_grad/Reshape&quot;\\n  input: &quot;gradients/loss/mse_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/mse_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/mse_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/mse_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/mse_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/mse_grad/Prod&quot;\\n  input: &quot;gradients/loss/mse_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/mse_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/mse_grad/Tile&quot;\\n  input: &quot;gradients/loss/mse_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Square_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/loss/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Square_grad/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;loss/sub&quot;\\n  input: &quot;gradients/loss/Square_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/Square_grad/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/mse_grad/truediv&quot;\\n  input: &quot;gradients/loss/Square_grad/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;predictions&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/loss/sub_grad/Shape&quot;\\n  input: &quot;gradients/loss/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/Square_grad/Mul_1&quot;\\n  input: &quot;gradients/loss/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/sub_grad/Sum&quot;\\n  input: &quot;gradients/loss/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/loss/Square_grad/Mul_1&quot;\\n  input: &quot;gradients/loss/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/loss/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/loss/sub_grad/Neg&quot;\\n  input: &quot;gradients/loss/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/loss/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/loss/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/loss/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/loss/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;MSE/tags&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;MSE&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MSE&quot;\\n  op: &quot;ScalarSummary&quot;\\n  input: &quot;MSE/tags&quot;\\n  input: &quot;loss/mse&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.1784179106002547&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph()) #lossがちゃんとtensorboardの中で省略表示されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name) #node名の名前を見ることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#バカ正直にReLUの出力するグラフを2つ作る\n",
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "#z2にすべきところをz1のままにしてしまっている。\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# 間違いをなくすためにもReLUを組み立てる関数を作る\n",
    "def relu(X):\n",
    "    '''\n",
    "    Xが入力されたら、ReLUを返す関数。\n",
    "    '''\n",
    "    w_shape = (int(X.get_shape()[1]), 1) #{特徴数}行,一列\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())\n",
    "#ログファイルにグラフ構造書き込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "# ReLUのグラフを作る関数を定義してしまう。\n",
    "def relu(X):\n",
    "    '''\n",
    "    reluという名前スコープでReLUを定義。\n",
    "    '''\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tensorboard --logdir handson-ml-master/logs/\n",
    "```\n",
    "を実行するとちゃんとうまく定義できていることが確認できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a **`threshold`** variable the classic way, **by defining it outside of the `relu()` function** then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    # thresholdをreluの外でthresholdを定義して引き渡す。\n",
    "    #しかし、いつも引数という形で渡さなければいけないのでめんどくさくなってくる\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"): #thresholdという文字列がreluというオブジェクトの属性にあるならTrueを返す関数\n",
    "            #つまりthresholdがなければreluにすれっしょるどを作る\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の方法は面倒。TensorFlowには少しクリーンでモジュール性の高いコードを作るためのオプションがある。\n",
    "* 考え方\n",
    "    * if 共有変数がまだない, 作る\n",
    "    * else, 既存の変数を再利用する（get_variableという関数を使う。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    # reluのスコープ内において、relu/thresholdという名前の変数を作る\n",
    "    # 以前のget_variableでこの変数がすでに作られている場合は例外を生成する。\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True): \n",
    "    #すでに作られている場合は例外を発生させてしまうので、reuse=Trueにしないといけない\n",
    "    # 同じ識別子を使っての変数再割当はreuse=Trueをつかう\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\") # 既存変数を再利用する。\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"): #reluの名前空間の中にthresholdという変数を定義する。\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 上記の例はthresholdを関数の外で定義していた。ちょっと気持ち悪い\n",
    "# ここではreluが呼び出されたら共有する変数をつくって、それ以降は再利用するようにしている。\n",
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable　#最初に呼び出すとthresholdという変数を作る\n",
    "    scope.reuse_variables()  # then reuse it #再利用\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 上記と同じことをしている。\n",
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some café?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the autodiff content was moved to the [extra_autodiff.ipynb](extra_autodiff.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyyHcfrR6if0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20d332e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _moons_ dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. As we saw in chapter 4, this model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\boldsymbol{\\theta}(\\mathbf{x}) = \\sigma(\\boldsymbol{\\theta}^T \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\boldsymbol{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\boldsymbol{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in chapter 4, the log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\boldsymbol{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} \\log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-7  # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, y_proba)  # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code, it's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.79260236\n",
      "Epoch: 100 \tLoss: 0.3434635\n",
      "Epoch: 200 \tLoss: 0.30754045\n",
      "Epoch: 300 \tLoss: 0.29288894\n",
      "Epoch: 400 \tLoss: 0.28533572\n",
      "Epoch: 500 \tLoss: 0.28047806\n",
      "Epoch: 600 \tLoss: 0.27808294\n",
      "Epoch: 700 \tLoss: 0.2761544\n",
      "Epoch: 800 \tLoss: 0.27551997\n",
      "Epoch: 900 \tLoss: 0.27491233\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54895616],\n",
       "       [0.70724374],\n",
       "       [0.51900256],\n",
       "       [0.9911136 ],\n",
       "       [0.5085905 ]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, you may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision). See chapter 3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627450980392157"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXt0HOWR6H9lWUKyjb228CXJJZbDWTgEvwQ2jw3YcOKE\nV5aFQB4GAeYuxERalmQ3S9Y+DsGJ1yQh2UDYgMGEp63NOndjAgGbEF6JjSEHEzDGbK7JAUwci0QI\nUCzJD1mu+0fPyKNRd0/3TM9M90z9zukjTffX3TWfWl91VX1Vn6gqhmEYhpGLEeUWwDAMw0gGpjAM\nwzCMQJjCMAzDMAJhCsMwDMMIhCkMwzAMIxCmMAzDMIxAmMIwDMMwAmEKwzAMwwiEKQzDMAwjECPL\nLUCUHHbYYTp58uRyi2EYhpEYXnjhhXdUdWKQthWlMCZPnsymTZvKLYZhGEZiEJHtQduaS8owDMMI\nhCkMwzAMIxCmMAzDMIxAVFQMwzCMyqa/v58dO3awZ8+ecouSOOrr6zniiCOora3N+xqmMAzDSAw7\nduzg0EMPZfLkyYhIucVJDKpKV1cXO3bs4CMf+Uje1zGXlFEZdHTAaafB22+XWxKjiOzZs4fGxkZT\nFiERERobGwu2zExhGJXB0qWwYYPz06hoTFnkRxT9ZgrDSD4dHXDPPXDggPMziJVhFolhhMYUhpF8\nli51lAXAwEAwK8MsEiMPampqaG5uZurUqXz2s5+lr68v9DWuvPJKXn31VQBuuOGGIcc+9rGPRSJn\nsTCFYSSbtHWxb5/zed8+uP12ePnl3OeEsUiMRNK+pZ3JN09mxDdGMPnmybRvaS/oeg0NDbz00ku8\n8sor1NXVcfvtt4e+xo9+9COOPfZYYLjC2LhxY0HyFRtTGEayybQu0hw4ABdfHOycoBaJkTjat7Sz\n4OcL2N69HUXZ3r2dBT9fULDSSDN79mx+//vfA/D973+fqVOnMnXqVG6++WYAent7+dSnPsWMGTOY\nOnUqq1evBuD0009n06ZNLFy4kN27d9Pc3ExLSwsAY8aMAeDzn/88a9euHbzX5Zdfzk9/+lMGBga4\n9tprOeGEE5g+fTp33HFHJN8lKKYwjGTz7LMHrYtMXn31oOWQGa9ws0jMyqhIFj+xmL7+oS6jvv4+\nFj+xuOBr79+/n3Xr1jFt2jReeOEF7rnnHn7zm9/w3HPPceedd/Liiy/y6KOP8qEPfYjNmzfzyiuv\ncNZZZw25xre//e1Bi6W9fagSmzdv3qCC2bdvH0888QTnnHMOd911F+PGjeP555/n+eef58477+SN\nN94o+PsExRSGkWxefBFUna21FerqnP21tbBwoaMoFi06GK9ws0jMyqhI3up+K9T+IKQtglmzZjFp\n0iSuuOIKNmzYwKc//WlGjx7NmDFjuOCCC1i/fj3Tpk3j8ccf51//9V9Zv34948aNC3yfs88+myef\nfJK9e/eybt065syZQ0NDA4899hj3338/zc3NnHTSSXR1dfHaa6/l/X3CYol7RmXgZjmsWuUog2ee\nORivOPLI4RbJvn3g5jvu6IB582D1avjAB4r/HYxImTRuEtu7hxdinTRuUt7XTFsEmaiqa9ujjz6a\nF154gbVr17Jo0SLOOOMMvv71rwe6T319Paeffjq/+MUvWL16NRdddNHgvf7jP/6DM888M+/vUAhm\nYRiVgZflkP3ztNMOWiSZ24svul/TZlIllmVzlzGqdtSQfaNqR7Fs7rJI7zNnzhx+9rOf0dfXR29v\nLw888ACzZ89m586djBo1iksuuYR/+Zd/4be//e2wc2tra+nv73e97rx587jnnntYv379oII488wz\nWb58+eA527Zto7e3N9Lv44cpDKMy8IplZBImXmEzqRJPy7QWVpy7gqZxTQhC07gmVpy7gpZpLZHe\n5/jjj+fyyy/nxBNP5KSTTuLKK6/kuOOOY8uWLZx44ok0NzezbNkyvva1rw07d8GCBUyfPn0w6J3J\nGWecwa9//Ws+8YlPUJdytV555ZUce+yxHH/88UydOpWrrrqK/fv3R/p9fFHVitlmzpypRgY7d6rO\nmaPa0VFuSYpL9vfcuVO1vt7NjlCtq1Nta8t9zdZW1dpa55za2mDnGEXn1VdfLbcIicat/4BNGnCM\njdTCEJGrRWSTiOwVkXtztP0nEXlbRLpF5G4ROSTj2GQReUpE+kTkdyLyiSjlrBqqxaWS/T3d3FNp\nvOIVmaSti7SroL/frAzDIHqX1E7g34C7/RqJyJnAQmAuMBk4EvhGRpMfAy8CjcBi4L9FJNCas0aK\nanGpuH1PL/dUc7N3vCKTpUsPxj3S7N9f+YrXMHIQqcJQ1TWq+jOgK0fT+cBdqrpVVd8DlgKXA4jI\n0cDxwPWqultVfwpsAS6MUtaKJ2xyWtJqK6XlXbRo+PfMnGqbK7DtxrPPHrQu0vT357ZMDKPCKVfQ\newqwOePzZuBwEWlMHXtdVXdlHZ9SQvmSTT7JaeVwXxWipJYuhfXrnamzUSfhrV0L9fVD9zU0wL33\nJkupGkbElEthjAG6Mz6nfz/U5Vj6+KFuFxKRBam4yabOzs7IBU0kYZPTst06mzeXZmDMV0ml5VUd\n7jqKIgnPq/9aWqojJmQYHpRLYfQAYzM+p3/f5XIsfXwXLqjqClWdpaqzJk60MAfg7sP3C/Zmu69K\nMTAWEmMpNKidC6/+e/XVyo8JGYYP5VIYW4EZGZ9nAH9S1a7UsSNF5NCs41tLKF+yCePDd3Nfbd1a\n/IEx3wKA2fKC4y7q6Agfq/DCrf9aW51yI2HlNSoKEeErX/nK4Ofvfe97LFmyJPL7xLXsedTTakeK\nSD1QA9SISL2IuJUfuR+4QkSOFZHxwNeAewFUdRvwEnB96vxPA9OBn0Ypq5HC7229WANjIQUAFy6E\nvXvd5SxW4N4KFiabCJ+LQw45hDVr1vDOO+9EIJg3cS17HrWF8TVgN86U2UtSv39NRCaJSI+ITAJQ\n1UeBG4GngO2p7fqM68wDZgHvAd8GPqOqFqAoBn4Z0l4DY6H/gIUUAHzkEeeNP1vOjRuLF7iPsmBh\n0majVQIRPhcjR45kwYIF3HTTTcOOdXZ2cuGFF3LCCSdwwgkn8Mwzzwzu/+QnP8nxxx/PVVddRVNT\n06DCOf/885k5cyZTpkxhxYoVAPEuex40wy8Jm2V6F0hrq5MJnSszurVVdcSI/LOfm5vds7Cbm/3P\ny8zgbmgYmsHud6xQ8pXXjUL7rsoJnekd8XMxevRo7e7u1qamJn3//ff1u9/9rl5//fWqqnrRRRfp\n+vXrVVV1+/bteswxx6iq6j/8wz/oDTfcoKqq69atU0A7OztVVbWrq0tVVfv6+nTKlCn6zjvvDN4n\n+76qqmvWrNHLLrtMVVX37t2rRxxxhPb19ekdd9yhS5cuVVXVPXv26MyZM/X1118fJn+sMr2NhBMk\nWB5FQqBbjGDnThg7NvfUX6+4RzEXRXrxRSeGMWIEtLXlFyvp6ICTT66OZMo4UYTnYuzYsVx22WXc\ncsstQ/Y//vjjXH311TQ3N/N3f/d3/OUvf2HXrl1s2LCBefPmAXDWWWcxfvz4wXNuueUWZsyYwckn\nn8wf/vCHnKXKy172PKhmScJmFkYJyLRCgtZlCnpdvzdvt/pQ6TdGv2NREMVbamurc/6IEe59l1kP\nq1pqgOVBKAujCM9F+k2/q6tLm5qadMmSJYMWRmNjo/b19Q07Z/r06UPe9sePH6+dnZ361FNP6Smn\nnKK9vb2qqnraaafpU089NeQ+2fdVVb3kkkv0wQcf1IsuukgfeughVVW94IIL9NFHH80pv1kYRunw\nCv4WmrcRxGrxiyMUe1Ekv7fUIDGJjg64O1UtJ32d7PhQpp+9WmqAFZsiPhcTJkzgc5/7HHfdddfg\nvjPOOIMf/vCHg5/T62aceuqp/OQnPwHgscce47333gOgu7ub8ePHM2rUKH73u9/x3HPPDZ4b27Ln\nQTVLEjazMIqMV4xjypTC/PJBrBa/OEKUMYZscr2lBolJpNt4Vc7NvEd9veohh0RvJVUIoSyMIjwX\nmW/6b7/9tjY0NAxaGJ2dnfq5z31Op02bph/96Ef1qquuUlXVP/3pT/rxj39cjzvuOP3yl7+sH/zg\nB3XPnj26Z88ePeuss3TatGn6mc98ZoiF8dWvflWPOeYYvfjii4fdd9++fTphwgS9/PLLB/cNDAzo\nokWLdOrUqTplyhQ9/fTT9f333x8mf6EWRtkH+Sg3Uxg52LlT9aSTVE8+Ob+ByOsfUCT/Aa7Y7qRC\n8ZsIEMRV5VdqPT14Zd5jxAhvt5WRyPLme/bs0f7+flVV3bhxo86YMaNssphLygjO0qXwm9/Ac8/l\nZ5YXI6Et7mts+00ECBJQdft+dXUHg+dr1w518x044O22MhLJW2+9xQknnMCMGTO45ppruPPOO8st\nUv4E1SxJ2MzC8GHnzoOujrTro9C3+Cisg2K6k4pJ0O+e6/u5WTDZ1sz8+RYET5FECyNOmIVhBGPp\n0qElu/fuheOPL+ztNQrroNBS5OUi6HfP9f1yLS27bx88/PDBILgl/uGMcUZYoug3UxjVQHqWTuYA\np+rsX7gw/+uGLXJYSUT13b0USnrbuRN6ew/OIFu0qKpnUNXX19PV1WVKIySqSldXF/XZZftDIpXU\n8bNmzdJNmzaVW4z40dYGd9zhXjOqpgZ27IAPfKD0clUK6f794hfh1lujv/ZddznKqK7OsWIGBpyC\ni6+/XnV/t/7+fnbs2MGePXvKLUriqK+v54gjjqA2HXNMISIvqOqsINcwhVENHHccpOaEu9LWFv1A\nF2c6OmDePFi9uvABt6MDjjwS9uyJfhDPvHY2dXVw5ZXV9XczikIYhWEuqbhQTN90pttj587hq8ml\nZ+JUi388ysS4YpYkybXuh82gMkqMKYy4UKrsXr/y4NWQYZxPLSwvRVrssue5AuJxmn5sVAWmMOJA\nFAX9guJVHvxXv6qOwnj5WAReirTYOSSZRQ8bG4cfr5YJBkZsMIURB4rh1nB7K+7ocGbcwPBV6ubM\nKZ5rJS7kYxH4KfNizxLLvHdf39C/V3pbu9b971wNrkWj9ARN2EjClsjEvUKT37wqm7rVOPKq2RT3\n8hxREXS9D69zSl2qI/PeIk4Cn1sbt7+zrblhBASrJZUg8hnEss/PHhzcahz5KYVCZUgKYbPKy6lI\n3e5dU5N70ahiLiRlVCRhFEbUa3pPEJEHRKRXRLaLyMUe7dallmxNb/tEZEvG8TdFZHfG8ceilDNW\nFOLW8HKXuLm4/Pzt1ZKAFzarvJx1rrzunZlomevvXKmuRaN8BNUsQTbgx8BqYAxwKtANTAlw3tPA\n1zM+vwl8Iuz9E2lhFIKbu8TrrXjKlHBv10Z561x53fuww5zjbn/n+vrqcC0akUIIC2NkVIpHREYD\nFwJTVbUH2CAiDwGXAp71J0RkMjAb+D9RyVIVeAVw02UkMhkYcIKgr7xSejmTTDnrWb34onviXm+v\nY0m6WSBuU3AHBpyaYb/9bdVlhRvRE6VL6mhgQFW3ZezbDEzJcd5lwHpVfSNrf7uIdIrIYyIyw+tk\nEVkgIptEZFNnZ2d+kicRL5fFww9Xh3upGgjrRswsjZ5m3z5H8ZhryoiAKBXGGBwXVCbdwKE5zrsM\nuDdrXwswGWgCngJ+ISJ/5Xayqq5Q1VmqOmvixIlhZU4uXnGHD384mdVfjeH4xZZyFS3UrKz+Ss6t\nMUpGlAqjBxibtW8ssMvrBBE5FfgA8N+Z+1X1GVXdrap9qvot4H0ct5WRJqllwY3gFPo3tgC4ETFR\nKoxtwEgROSpj3wxgq88584E1qZiHHwpIgfJVLpaoZWRT7LIlRlUSmcJQ1V5gDfBNERktIqcA5wEr\n3dqLSAPwWbLcUSIySUROEZE6EakXkWuBw4BnopK14qiGGlBGMNIvD4sWxXvpWyORRF0apA1oAP6M\nM8W2VVW3ishsEcm2Is7HiXE8lbX/UGA58B7wR+As4GxV7YpY1sqglHWojPiTfnl45BGb/GBEjq2H\nkXSyF9ixNRKql2KuzWFULLYeRrVgfmojk1IEuS1eVtWYwkgy5SxdYcSLUr08WLysqjGFkQS83uqq\npQaUkZtSvDxYvKzqMYWRBLze6tIL7NTVOZ/r6pyYhuViVB+leHmwvI6qx4LecccvkOlWa8iCndVF\nRwfMmwerVwf7m4dtn3mePWsViQW9Kwm/tzqLYRhhYwr5xiD8njULhFcNpjDiTK5ApsUwqpuwMYVC\nYhB+z5oFwqsGUxhxpaMDZs70tyCsnlR1EzamUEgMwutZW7vWAuFVhCmMuJI29c2CMNzwsz7dXETF\nmnZrgfCqwhRGHEn/c4MTWOzocEpVz5nj/G4WhOEXU3BzERUj3mWJo1WHKYw44rVWs5uf2AKO1YlX\nTOFXv3J3EfnFIPJ9hmzSRdVhCiNuuL213X23t5/YAo7hqQQl6xVTmDNn6MvG8cc733Pt2oOLKaVp\naIB16/J/hmzSRdVheRhxI7OYYJoRKb1+4MDQAoNWbC4/2trgjjvg0kvhjTfC5yTEFbdcCYDLL3ee\nj+znqq4OLrrI+f72DFUtloeRZHKt1ZzpJ7aAY3gyp5auWgXr11dOv7m5iABWroRf/9rdGnj4YXuG\njMCYwsiHYro0sl0NmaU/0gwMwMKFFnDMh2wlq1o5/eb2sgHO9zzttOHrfZ98MvT0DH+GNm9OvsvO\nKAqmMPIh6gC033lefuLMN8M09oboT3Z8KE2l9Fv6ZWPnzuHxCrfY13PPQX//0HYDA9DSYnExwx1V\njWwDJgAPAL3AduBij3ZLgH6gJ2M7MuN4M/AC0Jf62Rzk/jNnztSis3Onan29857W0KDa0XHwWGur\n6ogRqm1twa4zZ45zfuZ5mfv9aG52C3k6+w13WltV6+rc+y37b5lk3L5nXd3B5zLzGXbbRCqvTwxP\ngE0adIwP2jDQxZxlWVcDY4BTcZZgneLSbgmwyuMadSll80/AIcA1qc91ue5fEoWR+c/o9U8Y5B8t\nrSTmzx963vz5wZVO5nWCtq9mvJRs9t8y6eR6mfB6hnMdMyqSsigMYDSwDzg6Y99K4Nsubf0Uxhk4\na3lLxr63gLNyyVB0heH2ZpZWDmH+0TKvU1OjWlvr/F5b63wOqnTCKinDoZqtM79n2O9Y0GsHsY6N\nWBFGYUQZwzgaGFDVbRn7NgNTPNqfKyLvishWEWnN2D8FeDn1RdK87HOd0uGVqBQ2AJ0deE37kfv7\nnc/p/cWsDVTNVHMNrlwZ4oXExSwnqOKJUmGMwXFBZdINHOrS9ifAR4GJwBeAr4vIRXlcBxFZICKb\nRGRTZ2dnvrIHI4oAtFfgNZtcSsfKMhj54JdsV0ginq3GVxVEqTB6gLFZ+8YCu7IbquqrqrpTVQdU\ndSPwA+AzYa+TutYKVZ2lqrMmTpxY0BfIideb6Yc/HPwfzWuuvBt+b3dWlsHIBz/rau3ag/XKwlpe\nZu1WBVEqjG3ASBE5KmPfDGBrgHMVkNTvW4HpIiIZx6cHvE55COPi8Jornz0NEvzf7qwsQ3gqoSRI\nMcnXpWTWbtUQmcJQ1V5gDfBNERktIqcA5+EEvocgIueJyHhxOBFnJtSDqcNPAwPANSJyiIhcndr/\nZFSylhUv5bJ7dzi/ejX64Qsd8M3H7k0hLiWzdquGqBP32oAG4M84U2xbVXWriMwWkZ6MdvOA3+O4\nme4HvqOq9wGo6j7gfOAy4H3g74HzU/srD3vrDU4hA7752P0pxKVk1m5o2re0M/nmyYz4xggm3zyZ\n9i3t5RYpEFZ8sNykC+F98YtOQUHDnUILLWYWdcws4Gi4Fy20QoRFo31LOwt+voC+/r7BfaNqR7Hi\n3BW0TGspuTxWfDAp2FtvcAp5AzYfuz/mUiopi59YPERZAPT197H4icVlkig4pjDKic0sCUahA74N\niP6YS6mkvNX9luv+7d3bY++aMoVRLuytNziFDvg2IPqTnkDR2uqsvdLWVvkTKMrIpHGTPI8t+PmC\nWCsNUxjlwt56g1PogF+NM8rCYu7RkvHXE/7a81jcXVOmMMqFvfUGxwb84mPu0ZLQvqWdJ9/wzxDY\n3r09trOnbJaUYVQ7QWdJdXTAvHmVs6RtGZh882S2d28P3F4QFKVpXBPL5i4ryiwqmyVlGEZwgrpH\nXfJgkppPUC68At5eKM4L/fbu7bGIb5jCAEueM6qbIO5RlxhHOp9ge/d2FI3NoBZn/ALeuYhDfMMU\nBljJCKO6CRIjcolxJDmfoFwsm7uMUbWjhuwThNZZrTSNa8p5flgLJWpMYdjskMrFLEdfAruTPKaA\n793h7osv96AWZ1qmtbDi3BU0jWtCEJrGNbHygpXc9qnbWDZ3GbUjan3PL8RCiQJTGDY7pHIxy9GT\nUO4kjxjHd54b43rtcg9qcadlWgtvfvlNDlx/gDe//OaQQPbQIt1DGVU7imVzl5VCRE+qW2FY8lzl\nYpajL6HcSR4xjr99p3GYeyUOg1pSWfzEYvYNuNdYbRrXVLZaU5lUt8Kw5LnKxSxHX7zcRpn7B11W\n529m8k1NtL+8akiMY8Lv3hzmXonDoJZUvP4mggyzRMrFyHILUFYsea4y8bIcr7vO8gdSTBo3yTUf\nIO1Oyq6omnZZAUMGrpZpLbEYyCqBXH+TOFDdFoZlEFcmZjnmxG22TqY7yWZA5U++uSm5/iZxoLoV\nhlGZmOWYE7fZOpnupCAuK2M4heSm5PqbxAErDVIsrIyCkWC8Slg0jWvizS+/WXqBEkIS+61spUFE\nZIKIPCAivSKyXUQu9mh3rYi8IiK7ROQNEbk26/ibIrJbRHpS22NRylkSbEqnkWCS4B6JI5VumUXt\nkroV2AccDrQAy0Vkiks7wVmzezxwFnC1iMzLanOuqo5JbWdELGdxsSmdRsJJu0caGxoH9zWMbAh9\nnWqrNeUVoI5T4LoQIlMYIjIauBC4TlV7VHUD8BBwaXZbVb1RVX+rqvtV9f8BDwKnRCVL2bEpnUaF\nsHv/7sHfu3Z3haoVVY21pirdMovSwjgaGFDVbRn7NgNuFsYg4qQ2zga2Zh1qF5FOEXlMRGb4nL9A\nRDaJyKbOzs58ZY8OSwY0KoRCZ0pV40yrJASuCyFKhTEG6M7a1w0cmuO8JSk57snY1wJMBpqAp4Bf\niMhfuZ2sqitUdZaqzpo4cWIeYkdMFFM6rQaSEQMK9cdXuj/fC7/SH0knSoXRA4zN2jcW2OV1gohc\njRPL+JSq7k3vV9VnVHW3qvap6reA93GskPgTxZROC5gbMaBQf3yl+/OrkSgVxjZgpIgclbFvBsNd\nTQCIyN8DC4G5qrojx7UVJ1Aef9auhTlzHCuhtRVGjIC2tuDJgBYwN2JCof74SvfnVyORKQxV7QXW\nAN8UkdEicgpwHrAyu62ItAA3AJ9U1dezjk0SkVNEpE5E6lNTbg8DnolK1qKStg4WLsxv4LeAeX4E\ndeOZuy8whfrjK92fX5WoamQbMAH4GdALvAVcnNo/G+jJaPcG0I/jxkpvt6eOTQFeTl2jC3gCmBXk\n/jNnztSysnOnan29U2Ckpka1ttb5va5Ota0t3PnpraFBtaOj+LInndZW1REj/Pt5507VD35QVSTY\n38OIhFUvr9Kmm5pUlog23dSkq15eVW6RjAyATRp0jA/aMAlb2RVGa6ujHNwqVImobt4c/vygyqaa\nyVS0fgr2sstMEYek0MF+1curdNSyUcoSBrdRy0aZ0ogRYRSG1ZKKiuzptNmowsWuie8HsRpI4Ui7\nlxYtyu3G6+iA9oz5//v3m7svB7nyKIIk5VXj1NpKxmpJRUVbG9x1l7fCABCBnTuttlRUtLXB7bc7\nEwsGBg7ub2iA118f2s/z58P99w89362dMYhfXaRlc5cNKX8OTkA7O0Yx4hsjUIaPMYJw4PoDw/Yb\npadstaSqGjfrIE1t7cGf9lYbDS+95CgL1aHKAoZbD9nWhVc7Ywh+eRRBLQebWltZmMKICre1NY4+\n2jnW3+/8TGd9b95sM3UK5ZJLnD52o79/qBtv6dLhSsWtnTEEv8E+aFKeTa2tLExhFItf/hK2bRu+\nf2AAWlosMa8QXnoJtg5N7+mrgd01qd9Hwk+/f+XgsXefXOt+neZmWyzLB7/BfkLDBNdzsvfb1NrK\nwmIYxWLCBHjvPfdjIs7bsfnQ82Pq1GEKYz+AwEiFPTXwk5PHcNmGXcOWGgV3X7vhTvuWdhY/sZi3\nut9i0rhJLJu7jJZpLRx242F07e4a1r6xoZF3vvpOGSQ18sViGOXml790VxZPPOFkf6djGpaYF56O\nDnj11WG7R+IoC4D6AfjMb3rg7bdtlk6ReHf3u677u3Z3VUUZ82rFFEYx+Pzn3fdfcIF3JVvLQA7G\n0qUHFW6K/cD+rMIxNeq09fO1V9taDWHxm1brF7SuhjLm1YopjKjp6PB2RXV3e1eytYKDwXCZjZZp\nXaQ5ZADYuNFzYJvQMKHq1moIi5915hbfcGtnVBamMKJm6VKoq/M+7paY96tfWcHBoGTMRmt/eRWj\nl41CljC4jVgitD3c6rR58UXPwC1grqoc+FlnmcHssOcbycUURtT45WOAMzMne/rtnDlWcDAP3Gbg\nrLxgJbd96jbfNivOXeHpg7dB7iC5cijS6z54KQ3Ltag8TGFETfoNuLX1oKVRV+dkJafeeodgK/QV\nRJDFarLbAIwQ90ffBrmDBM2hsFyL/EhiDM0URjEIowSiWKHPCEw6kDugwxP5sge5JP5DR0nQHArL\ntQhPUtc7tzyMYuBWV6quDq68Em69dWjb445zEtGysaSyouBVH6lGarjv0/cNDnKWv2EUE786XWkr\nuFSEycMwhVEMTAnEFq9ieOD8s6YT1Hr29VhimlE04lSU0RL3yo1bXSm3+IVRcrxiFIIMcQ+4KQtw\nEtPi7jYw4k9URRlL7TaNVGGIyAQReUBEekVku4i4LgAhDt8Rka7UdqOISMbxZhF5QUT6Uj+bo5TT\nqF7cArSCeFodbtjUW6NQopgoUI44SNQWxq3APuBwoAVYLiJTXNotAM4HZgDTgb8FrgIQkTrgQWAV\nMB64D3j/Vh1wAAAVVUlEQVQwtd8wCiI7QNvY0BhKWQCuvudqo9onBBSK10QBIHC/lqPsTWQxDBEZ\nDbwHTFXVbal9K4E/qurCrLYbgXtVdUXq8xXAF1T1ZBE5A7gHOCK1fCAi8hawQFUf9ZMhNjEMIxG4\nBbaDUCM17P/6/iJJFX9sQkBxCNuvUcVByhXDOBoYSCuLFJsBNwtjSuqYW7spwMs6VJO97HEdw8gb\ntze0NH5lL9ym5FYTVtCxOITt13IsThWlwhgDdGft6wYODdC2GxiTimOEuQ4iskBENonIps7OzrwE\nN6oTv6xuv7IXfuUwqoGgiycZ4Qjbr+VImIxSYfQAY7P2jQV2BWg7FuhJWRVhroOqrlDVWao6a+LE\niXkJblQnXm9iTeOaaJnWYhnMHtiyq8UhbL+WI2EySoWxDRgpIkdl7JsBbHVpuzV1zK3dVmB65qwp\nnMC423UMI29yKYQw/5DVFAQ2RVoc8unXIKVxIkVVI9uA/wJ+DIwGTsFxJU1xafdF4H+A/w18CEcZ\nfDF1rA7YDnwJOAS4OvW5Ltf9Z86cqYax6uVV2nRTk8oS0aabmnTVy6siaet3jVHLRilLGNxGLRuV\n17WSQhT9ZgynHP0KbNKAY3ykmd4iMgG4G/gk0AUsVNX/FJHZwDpVHZNqJ8B3gPTCyz8C/jUlPCJy\nXGrfsSnFcoWq5sx6s1lSRjlm8MSpzINhhMVKg0RBRwfMmwerV9ua2wmiHIN3nMo8GEZYrDRIFNgK\neImkHDN4LAhsVAumMNxIlye3FfASRzkGbwsCG9WCKQw3MteocFuboqMDTj4Z/uZvTJnEjHIM3rYe\nhFEtWAwjm44OOPJI2LPn4L6GBnj99YOxjLY2WL784O/Za1wYZaV9SzuLn1g8WKp82dxlNngbhgcW\n9C6EXIsfdXTARz4Ce/c6x+rr4Y03LDBuGIYvcX2RsaB3ITz77FBlAc7njRud35cuhf7+occsMF71\nJD1xL+nyx52kLsmajVkYYci2LtKYlVHVJL16a9LlLyX5WglxztUxC6NYZFsXaczKqGqSXr016fKX\nikKshEop2GgKIwzPPntw9lQmBw4cdFkZVUfSBwMvObd3bzf3VAaFKFavad2KJqqPTWGEwWutbluv\nu6pJeuKen5xJ9bWHJUgMp5AXA7fp3mmS1MemMAyjQPLN/YhLoNlvMINkuafy6dOgriYvxTpCRuS8\nT2aujhtJ6WNTGIZRIPkk7sVp1kyuwQyS4V7Lt0+Dupq8FOuADgS6T7oUuSCux5PQxzZLyjDKQFxn\nzcRVriCElT0948ntHHAvHtm+pZ35D8x3XaY3aB/FrY9tlpRhxJy4BsqTXBcrTJ9mWiNeuLmgWqa1\ncEDdKxAH/dsluY9NYRhGxATxo8c1UJ7kulhh+tTNDZWJ3wBe6N8uyX1sLinDiJCgSXCWLBc9YfrU\naw0TcFxDfgl5lfa3M5eUYZSJoAHUJL9lxpUwfeplDaTjCH5/h2r+20ViYaSWZr0LOAN4B1ikqv/p\n0fZaYD7QlGp7m6p+N+P4m8DhQDqqtFFVzwgih1kYRrmx1feSQRRWQlyLCYalHBbGrcA+nIG+BVgu\nIlO85AMuA8YDZwFXi8i8rDbnquqY1BZIWRhGHIhrbMIYSqFWQpApvHHJs4mSgi0MERkNvAdMVdVt\nqX0rgT+q6sIA59+SkuMfU5/fBK5U1cfDymIWhlFuKs2/bbiTa2pskp6DUlsYRwMDaWWRYjPgZWEM\nIiICzAa2Zh1qF5FOEXlMRGbkuMYCEdkkIps6OzvDym4YkZJvEl+lvYkmjbB/g1xTeCu1oGMUFsZs\n4P+q6gcy9n0BaFHV03Oc+w3gfOBEVd2b2ncK8Fsc19WXUtsxqvp+LlnMwjCSRpLeRCuNzMQ9QYbE\nnmqkBkU5oAeokRoWzFzAbZ+6bfB4LgsjSbGsSC0MEXlaRNRj2wD0AGOzThsL7Mpx3atxYhmfSisL\nAFV9RlV3q2qfqn4LeB/HCjGM2BP2TbVS30TjTnbiXvbgPqADgwl6AzrA8k3LaXukbfB4ruQ7v7pT\nSbYkcyoMVT1dVcVjOxXYBowUkaMyTpvBcDfTICLy98BCYK6q7sglAngUXzGMEpJLGeRTyyiuGd+V\nTq7EPTdWvLBi8Pdcrke/ulPlrh1WCAXHMFS1F1gDfFNERqdcSucBK93ai0gLcAPwSVV9PevYJBE5\nRUTqRKQ+NQX3MOCZQuU0jEIIogzysRZKNavK4iRDyUchZ9ePShcTPHD9gWG5G9kKpUZqhl0viZZk\nVNNq24AG4M/Aj4FWVd0KToxDRHoy2v4b0Ag8LyI9qe321LFDgeU4s67+iDPt9mxV7YpITsPIiyDK\nIB9rwcu1cc5R50Q2wMepMq4b5VBm+ShkryqzXmQqlELrT8WFSBSGqr6rquer6mhVnZSZtKeq61V1\nTMbnj6hqbUaexRhV/WLq2FZVnZ66TqOqzlVVi2IbZSeIMsjHWnBzbcyfMZ/7Nt8X2QBfrjhJEEVQ\nLmWWaw0QN0bXjc77fpWSn2OlQQwjAEH+4c856hzXNl7702S7Nta+tjbSAb4ccZKgiqBcyixbUTc2\nNFJXU+d7Tu++3rzvl+QKtZmYwjCMAAT5h1/72lrXc732exH1AB9E2UXtFgqqCMoZ9M9U1GPqxrBv\nYJ9ve7d+DDIRYvLNk7l0zaU0jGygsaEx0fWnTGEYRgCCJOTlGvyCDsqFui+y73POUef4KrtiuIWC\nKoKw37VY8Y5cCsrNGsjVb9nHu3Z3sXv/blZesDJngcO4YgrDMALiNysG/Ae/MINyIe4Lt/vct/k+\n5s+Y76nsiuEWCqoIwnzXYsY7/JSxlzWQq98qMcfGFIZhRITf4Bdm8CikMJ7Xfda+ttZT2RXDLRRU\nEYT5rsUcgL3kXXXBKk9rIFe/VWKOzchyC2AYlULmG3t2yetL11zqeo7X4NEyrSUvl0U+g9SkcZNc\ny1wUMoPHry/c2gb5rl7fYXv3dtq3tBfk4gkjb5pc/VaMfi03tuKeYZSAXLWHin2fxoZGxtSNcR0M\nk1LPyuu7QXnkzdVvSelXW3HPMGJGvnGJsEFet/vU1dTxl71/GeL7v3TNpYO1kZKygpxf7kQ5YgO5\n+i0p/RoGszAMo0SEXaEt3zfU7Pv07Ouha/fwYgmCsPKClYkawNq3tHPJmktcj8WxEmwSCGNhmMIw\njJgSlRvLq9R2PteKA6Vy71UL5pIyjArAL8gbJgfBL8hajhk7heZSlMq9ZwzHFIZhxBS/gT5MDsKy\nucs8C+eVcsZO+5Z2DrvxMC5Zc0lBuRT5rmqYK8nOlEluzCVlGDHFLYaRTVA3TNsjbSzftHzIvrqa\nOu4+7+6SxDByfZeo3UlB4zhN45pYNndZImYzFQtzSRlGhdAwssH3eFCX0imTTqF2RO2QfaV8Wcy1\nYFGUrjE3a8JNWaTvW4kZ2cXCFIZhxJD0oOc10KUJ6lJa/MRi+g/0D9nXf6CfL637Ut4yhiGXQojS\nNRZmNb1J4yZVZEZ2sTCFYRgxJMigl11A0M8H7zX4de3uQr4hRffb+ymEQsp8u33voAN9+r6VslZF\nKYhMYYjIBBF5QER6RWS7iFzs03aJiPRnrLjXIyJHZhxvFpEXRKQv9bM5KjkNIwn4DXrZgd4gRfly\nDX7FXrho2dxlnutNpN0/Ye/t9b0nNExwbd/Y0OgaKK+UtSpKQWRBbxH5MY4CugJoBh4BPpZeqjWr\n7RLgr1V1WAaOiNQBrwE3A7cBVwFfAY5SVd+C9Rb0NiqFMLkGQdr6JbxlUiM13Pfp+4oS7D3sxsN8\nXWxhA81+ZVB2798dKogdNqmykih50FtERgMXAtepao+qbgAeAtwrrvlzOk5RxJtVda+q3gII8PEo\nZDWMJBDmrTeID75lWguNDY057zugA0Wbbvru7nd9j4cNNHt973d3vxt62m2u0vWGQ1TVao8GBlR1\nW8a+zcBpPuecKyLvAh3AD1U1PedvCvCyDjV9Xk7tfzQieQ0j1oSpnhq0KuoPzv5Bzmm6MHTgzmyf\ndvlkyhcGLzkzCRNo9vve+Vb7NfyJKoYxBujO2tcNHOrR/ifAR4GJwBeAr4vIRflcS0QWiMgmEdnU\n2dmZj+yGEUuCvvXms/YE4JnMB8WZbupXPDBNmECzxR5KTyCFISJPi4h6bBuAHmBs1mljgV1u11PV\nV1V1p6oOqOpG4AfAZ1KHw15rharOUtVZEydODPJ1DKOiCJP5nFZCer2y8oKV1EiN6zWjnG7qtq41\nDFdYYQf7SqwGG3cCuaRU9XS/46kYxkgROUpVX0vtngEMC3h73QIGn56twFdERDLcUtOBWwNeyzCq\njnxcMOn2blnO6VUCC10AKDvDu2t31+BKdhBuwSKv72AKonRE4pJS1V5gDfBNERktIqcA5wEr3dqL\nyHkiMl4cTgSuAR5MHX4aGACuEZFDROTq1P4no5DVMJJEsWsc+b2lR+Hy8XNrWaA5eUQ5rXYCcDfw\nSaALWKiq/5k6NhtYp6pjUp9/DJwBHALsAG5LzYZKX+s44EfAscD/AFeo6ou5ZLBptUYlEYcV2wqd\nbupVWj1Oa1dU85RasPUwyi2GYURCJaz7ENV3KNagHgelXG6s+KBhJJBs95PXFNQk1TiKwq0VJJM9\nX6zwYDhMYRhGDHAbFOOwhkWhRDGTqZiDuhUeDEdUiXuGYRSA26CoKIIMiQHEOc/Ay21U6EymYg7q\nQZMeDQezMAwjBngNfoomIs+gmG6jYlaTteS/cJjCMIwY4DX4pYPD5Z56mmt6r5fbaP4D8wtWGsUc\n1C35LxzmkjKMGOC1TGgc3nSzZxK51ZTyspDSxQwz24YlTF2tfK9vCiIYNq3WMGJCXPMBgkyN9ZvV\nld3WiBc2rdYwEkZclQUECzrnKixY7llHxc6YrxZMYRhGmSlmwDgKggSd07EAr2KGI2RE2b5P3Ps3\nSZjCMIwyE/fksTDl0+/79H2ulkb2wkylJO79myRMYRhGmYl78ljY8ulelka5Bum492+SsFlShlFm\nkpA8FmYmUcu0Fi5d4746czkG6ST0b1IwC8MwykwlJo8VM9kuLJXYv+XCFIZhlJlKTB6L0yBdif1b\nLiwPwzCMohDnqcLGQWw9DMMwDCMQlrhnGIZhRE4kCkNEJojIAyLSKyLbReRin7brRKQnY9snIlsy\njr8pIrszjj8WhYyGYcQby8aOP1FNq70V2AccDjQDj4jIZlXdmt1QVc/O/CwiTwNPZjU7V1Ufj0g2\nwzBiTpACh0b5KdjCEJHRwIXAdarao6obgIcA94nYQ8+dDMwGVhYqh2EYycWysZNBFC6po4EBVd2W\nsW8zMCXAuZcB61X1jaz97SLSKSKPicgMvwuIyAIR2SQimzo7O8NJbhhGLLBs7GQQhcIYA3Rn7esG\nDg1w7mXAvVn7WoDJQBPwFPALEfkrrwuo6gpVnaWqsyZOnBhUZsMwYkScEv0Mb3IqDBF5WkTUY9sA\n9ABjs04bC+zKcd1TgQ8A/525X1WfUdXdqtqnqt8C3sdxWxmGUaHEKdHP8CZn0FtVT/c7nophjBSR\no1T1tdTuGcCwgHcW84E1qtqTSwRAcslpGEZyKfaqekY0RJK4JyL/hTOwX4kzS2ot8DG3WVKp9g1A\nB3CBqj6ZsX8S8GHgeRzr5x+BrwLHqGpXLjkscc8wDCMc5UjcawMagD8DPwZa08pCRGaLSLYVcT5O\nnOOprP2HAsuB94A/AmcBZwdRFoZhGEZxsdIghmEYVYyVBjEMwzAixxSGYRiGEQhTGIZhGEYgKiqG\nISKdwPC1GEvLYcA7ZZYhX0z28pFk+ZMsOyRb/ihkb1LVQFnPFaUw4oCIbAoaQIobJnv5SLL8SZYd\nki1/qWU3l5RhGIYRCFMYhmEYRiBMYUTPinILUAAme/lIsvxJlh2SLX9JZbcYhmEYhhEIszAMwzCM\nQJjCMAzDMAJhCqMAROTq1Gp/e0Xk3gDt/0lE3haRbhG5W0QOKYGYfvJMEJEHRKRXRLaLyMU+bZeI\nSL+I9GRsR8ZRXnH4joh0pbYbRaSsJfJDyF72fnaRKfBzHsNnPJDsInK5iAxk9fvppZPUVaZDROSu\n1POyS0ReFJGzfdoXve9NYRTGTuDfgLtzNRSRM4GFwFycFQWPBL5RTOECcCuwDzgcZ6XD5SLit7Tu\nalUdk7G9XhIpDxJU3gU4FZFnANOBvwWuKpWQHoTp63L3czaBnvOYPuOB/0eBZ7P6/eniipaTkcAf\ngNOAccB1wE9EZHJ2w1L1vSmMAlDVNar6MyBI+fX5wF2qulVV3wOWApcXUz4/UgtfXQhcp6o9qroB\neAi4tFwy+RFS3vnAv6vqDlX9I/DvWF/nTYjnPFbPOIT+H40VqtqrqktU9U1VPaCqDwNvADNdmpek\n701hlI4pwOaMz5uBw0WksUzyHA0MqOq2LJn8LIxzReRdEdkqIq3FFW8YYeR162u/71VswvZ1Ofu5\nEOL2jIflOBF5R0S2ich1IpJzRdJSIiKH4zxLbgvTlaTvTWGUjjE4i0alSf9+aBlkgeHykPrsJc9P\ngI8CE4EvAF8XkYuKJ94wwsjr1tdjyhjHCCN7ufu5EOL2jIfh18BU4H/hWIMXAdeWVaIMRKQWaAfu\nU9XfuTQpSd+bwvBARJ4WEfXYNuRxyR5gbMbn9O+7Cpd2OAHkz5YnLZOrPKr6qqruVNUBVd0I/AD4\nTDFk9yCMvG593aPlSzoKLHsM+rkQSvqMR4mqvq6qb6RcP1uAbxKTfheREcBKnBjY1R7NStL3pjA8\nUNXTVVU8tlPzuORWnCBsmhnAn4q1/GwA+bcBI0XkqCyZXNdhd7sFUMo39jDyuvV10O9VDArp61L3\ncyGU9BkvMrHo95RVfBfOZIkLVbXfo2lJ+t4URgGIyEgRqQdqgBoRqffxe94PXCEix4rIeOBrwL0l\nEnUYqtoLrAG+KSKjReQU4DycN5lhiMh5IjI+NWX1ROAa4MGYyns/8M8i8r9F5EPAV0hIX5e7n90I\n8ZzH6hmH4LKLyNmpGAEicgzOjKSy9nuK5TguynNVdbdPu9L0varalucGLMF5E8nclqSOTcIxEydl\ntP9n4E/AX4B7gEPKLP8E4GdAL/AWcHHGsdk4bpz05x/jzDTpAX4HXBMXeV1kFeBG4N3UdiOpMjhx\n6+s49rOL7K7PeUKe8UCyA99Lyd0LvI7jkqots+xNKXn3pGRNby3l6nurJWUYhmEEwlxShmEYRiBM\nYRiGYRiBMIVhGIZhBMIUhmEYhhEIUxiGYRhGIExhGIZhGIEwhWEYhmEEwhSGYRiGEQhTGIZhGEYg\n/j/1mvwOq2UFMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21100b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but you could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -5.14696757e-02,  4.44198631e-01,\n",
       "         2.64912752e-03,  1.97312424e-01, -1.36349734e-04,\n",
       "         8.76459084e-02],\n",
       "       [ 1.00000000e+00,  1.03201691e+00, -4.19741157e-01,\n",
       "         1.06505890e+00,  1.76182639e-01,  1.09915879e+00,\n",
       "        -7.39511049e-02],\n",
       "       [ 1.00000000e+00,  8.67891864e-01, -2.54827114e-01,\n",
       "         7.53236288e-01,  6.49368582e-02,  6.53727646e-01,\n",
       "        -1.65476722e-02],\n",
       "       [ 1.00000000e+00,  2.88850997e-01, -4.48668621e-01,\n",
       "         8.34348982e-02,  2.01303531e-01,  2.41002535e-02,\n",
       "        -9.03185778e-02],\n",
       "       [ 1.00000000e+00, -8.33439108e-01,  5.35056649e-01,\n",
       "         6.94620746e-01,  2.86285618e-01, -5.78924095e-01,\n",
       "         1.53179024e-01]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file, but in chapter 11 we will see how to store the training step directly as part of the model, using a non-trainable variable called `global_step` that we pass to the optimizer's `minimize()` method.\n",
    "\n",
    "You can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.16122366\n",
      "Epoch: 1000 \tLoss: 0.11903212\n",
      "Epoch: 1500 \tLoss: 0.097329214\n",
      "Epoch: 2000 \tLoss: 0.083697945\n",
      "Epoch: 2500 \tLoss: 0.07437584\n",
      "Epoch: 3000 \tLoss: 0.067502156\n",
      "Epoch: 3500 \tLoss: 0.062206898\n",
      "Epoch: 4000 \tLoss: 0.05802679\n",
      "Epoch: 4500 \tLoss: 0.05456298\n",
      "Epoch: 5000 \tLoss: 0.051708285\n",
      "Epoch: 5500 \tLoss: 0.04923774\n",
      "Epoch: 6000 \tLoss: 0.047167283\n",
      "Epoch: 6500 \tLoss: 0.045376644\n",
      "Epoch: 7000 \tLoss: 0.04381875\n",
      "Epoch: 7500 \tLoss: 0.04237422\n",
      "Epoch: 8000 \tLoss: 0.041089162\n",
      "Epoch: 8500 \tLoss: 0.03997092\n",
      "Epoch: 9000 \tLoss: 0.038920246\n",
      "Epoch: 9500 \tLoss: 0.038010746\n",
      "Epoch: 10000 \tLoss: 0.03715569\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797979797979798"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj+PgagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2113b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. You can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
